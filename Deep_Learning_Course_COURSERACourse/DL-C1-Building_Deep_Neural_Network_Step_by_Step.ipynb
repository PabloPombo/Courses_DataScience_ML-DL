{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building_your_Deep_Neural_Network_Step_by_Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Y = np.array([[1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
    "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
    "        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
    "        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
    "        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
    "        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
    "        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
    "        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
    "        0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 0.37421036,  0.39831362,  0.3285195 ,  1.1653739 ,  1.20013934,\n",
    "         1.30413494,  0.92421532,  0.93569327, -1.07086424,  0.53824266,\n",
    "         0.73249611, -0.0574624 , -0.21829824,  2.01478659, -0.57376742,\n",
    "         0.67046228,  1.44377404,  1.05886587, -0.75339574,  1.82218896,\n",
    "         1.85854176,  2.04398775,  1.9438202 , -0.98242283, -0.27320858,\n",
    "        -0.27557082,  1.02769267, -0.50512479, -1.05041356,  1.81625746,\n",
    "         0.79382874,  0.09655024,  0.12545292,  0.56692062,  1.0993748 ,\n",
    "        -0.3828155 , -0.14875237,  0.648378  ,  0.19180852,  0.91753678,\n",
    "         0.3927839 ,  0.68902246,  0.16415201, -0.46107949,  1.61961366,\n",
    "        -0.67465482,  0.56971698,  0.80326626,  1.78480398,  0.45230316,\n",
    "        -0.81683873,  1.47545687,  0.40168176,  0.66406859,  0.1300027 ,\n",
    "         1.20533281,  0.1940066 ,  0.20774852,  0.22360985,  0.73062656,\n",
    "        -0.61828491,  2.17756149, -0.09416353,  1.22798828,  1.07782389,\n",
    "         0.13605877,  1.09911226, -0.59702069,  0.40375215, -0.12480655,\n",
    "         0.88092572,  0.78791722, -0.75724687, -0.89442441, -0.14866122,\n",
    "         0.40777459,  1.58338141,  1.11800435,  0.879421  ,  1.97013221,\n",
    "         0.50272788, -0.59425066,  1.1627071 ,  0.47560131,  0.25403668,\n",
    "         1.88533565,  1.25260313,  1.01008212,  1.94441906, -0.63873362,\n",
    "         0.67464306,  0.71841857,  0.12864363,  1.88051705,  1.97127996,\n",
    "         1.74252831,  1.45443136, -0.13398391,  0.23539342,  1.14977788,\n",
    "        -0.31387159,  0.22885782, -1.1636081 , -0.51150563,  0.63525419,\n",
    "        -0.34758056, -1.44544255,  1.07240913,  0.6404266 , -0.20978387,\n",
    "         0.72314071,  0.21179767,  0.27796467, -0.08943785,  0.07527753,\n",
    "         1.42498105,  1.91703595,  0.45810422,  0.15552496,  0.0993877 ,\n",
    "        -0.79175036,  1.41074621,  2.19253364, -0.10455888, -0.41505482,\n",
    "         2.21317607,  0.60277536, -0.72382243,  0.24449181,  2.00319766,\n",
    "         0.55389283,  1.26958437, -1.03880921, -0.02415827, -0.66191421,\n",
    "         1.67788392,  0.96332335,  0.59599903,  0.79695955,  0.47452103,\n",
    "         0.64351064, -1.2945471 ,  1.76592759,  0.75580694,  1.07185701,\n",
    "         2.15205548,  2.04559436, -0.11061984,  1.30324706, -0.59812931,\n",
    "         0.42833239,  2.24120827, -1.01550035,  1.15476466,  1.31069856,\n",
    "         0.63213252, -0.91225325, -1.07832365,  2.04361255, -0.34925921,\n",
    "         0.12173854,  0.06277739,  0.03512991, -0.44424482,  0.64231338,\n",
    "        -0.09668098, -0.5814599 , -0.19727544,  1.59227859,  1.97807458,\n",
    "        -1.28841055,  1.86909388,  0.81020422,  1.39421542,  0.31153182,\n",
    "        -0.09750601,  0.6464494 , -0.9889099 ,  0.88497765,  1.36083421,\n",
    "         0.62158288, -0.51064864,  1.64020735,  0.60869811,  1.18287807,\n",
    "        -0.33194336,  0.43021617,  0.10422689, -0.79000203,  0.70209086,\n",
    "         0.52698137, -1.19392401, -0.67004451, -0.77801497,  0.24564474,\n",
    "         0.60865621,  2.04249187,  0.19637864,  0.50519793,  1.87640279],\n",
    "       [-0.14874283, -0.42458822,  0.77644768, -0.31421438, -0.53677287,\n",
    "         0.37005293,  0.0161565 ,  0.51065702,  0.15411431, -0.63119369,\n",
    "         0.57266453,  0.18119479,  0.02302089, -0.11420376,  1.07781337,\n",
    "        -0.33478218, -0.41610516,  0.60335563,  0.74122108, -0.07438342,\n",
    "        -0.04233788, -0.06419791,  0.55425955,  0.78443277,  0.96327721,\n",
    "         0.493452  ,  0.34276438,  0.78199354,  0.47171254, -0.58801549,\n",
    "         0.79553358,  0.73546954,  0.14345593,  0.84601396,  0.74177189,\n",
    "         0.73461303,  0.77735587, -0.5491141 , -0.26048943,  0.31951105,\n",
    "        -0.4060557 , -0.48996724,  0.77521598,  0.38012196, -0.56453605,\n",
    "         0.66291601, -0.39706353,  0.33722026, -0.47262628, -0.02503807,\n",
    "         0.24344619,  0.01700603,  0.83516548, -0.17601952,  1.0041116 ,\n",
    "         0.27156906, -0.20696082,  0.83852166,  0.24766095, -0.45378666,\n",
    "         0.70434784, -0.19677722,  0.52999727,  0.34494021, -0.1714286 ,\n",
    "         0.8530664 , -0.23988194,  0.73241247, -0.17910264,  0.94547033,\n",
    "         0.98801153,  0.69942459,  0.32106784,  0.26423199,  0.74380039,\n",
    "        -0.38983477, -0.05278656, -0.01696589,  0.40047856, -0.09446514,\n",
    "        -0.32307025,  0.31081159, -0.68428644,  0.13061163,  0.97275342,\n",
    "        -0.08292475, -0.56008291, -0.2172839 , -0.08756145,  0.61011456,\n",
    "        -0.12939516,  0.58252691,  0.80286194, -0.34612618,  0.05842789,\n",
    "        -0.03454704, -0.08230781,  0.84013255,  1.2577107 , -0.83271166,\n",
    "         0.93375479, -0.5165868 ,  0.3974282 ,  1.11418932,  0.61057816,\n",
    "         0.8192202 ,  0.51001741,  0.39404067,  0.06654629, -0.22582976,\n",
    "        -0.52877034,  1.12790442,  0.10198339,  1.13771589, -0.16992579,\n",
    "        -0.3446814 , -0.36685657,  0.79189869, -0.27815692, -0.25062697,\n",
    "         0.34721271, -0.23514301,  0.46200087,  0.00445406,  1.21015037,\n",
    "         0.49490053, -0.66804478, -0.26132165, -0.24774806,  0.79502087,\n",
    "        -0.63875662, -0.28905943, -0.07305462,  0.96429533,  0.05827149,\n",
    "        -0.62050166, -0.87046738,  0.82823067, -0.08049711,  0.00349654,\n",
    "         0.80988976,  0.27355327, -0.25566595,  0.51823343,  0.06793817,\n",
    "         0.01854438, -0.08205381, -0.04217318, -0.5045159 ,  0.98862523,\n",
    "        -0.25757112,  0.04460908,  0.41653527, -0.65178141,  0.05599921,\n",
    "         0.04851775,  0.31414483,  0.54513728,  0.40516248,  0.50254896,\n",
    "         0.27023905,  0.19431936,  0.85092786,  0.69339609,  0.6297686 ,\n",
    "         1.07156262,  0.76256197,  0.78419314, -0.53976562,  0.3834489 ,\n",
    "         0.11088435,  0.32123487,  0.6856705 , -0.49497416,  1.1629126 ,\n",
    "         0.82419249, -0.57501605, -0.48396409,  0.46325476, -0.23951833,\n",
    "         0.53872865,  1.22323174, -0.39390165, -0.45800236,  0.15393859,\n",
    "         0.21395214, -0.65652183,  0.99563246,  0.75665307, -0.25494785,\n",
    "         0.56840057,  0.07240814,  0.81093689,  0.72358347, -0.28670025,\n",
    "        -0.39128341,  0.30805663,  1.09054109,  0.88866631,  0.40204108]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.01788628,  0.0043651 ],\n",
       "        [ 0.00096497, -0.01863493],\n",
       "        [-0.00277388, -0.00354759],\n",
       "        [-0.00082741, -0.00627001]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[-0.00043818, -0.00477218, -0.01313865,  0.00884622],\n",
       "        [ 0.00881318,  0.01709573,  0.00050034, -0.00404677],\n",
       "        [-0.0054536 , -0.01546477,  0.00982367, -0.01101068],\n",
       "        [-0.01185047, -0.0020565 ,  0.01486148,  0.00236716]]),\n",
       " 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W3': array([[-0.01023785, -0.00712993,  0.00625245, -0.00160513]]),\n",
       " 'b3': array([[0.]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def initialize_parameters(layer_dims): #expects a tuple\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1)) \n",
    "        \n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters((2,4,4,1)) # X => 2 parameters , 2 hiden layers with 4 neurons, and one output\"\n",
    "parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Activation forward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward propagation\n",
    "def linear_forward(A, W, b):\n",
    "    \n",
    "    Z = W@A +b\n",
    "    cache = (A, W, b) # the ones that we used\n",
    "    \n",
    "    return Z, cache\n",
    "    \n",
    "\n",
    "#activation function\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        Z, AWb_cache = linear_forward(A_prev, W, b)\n",
    "        A, Z_cache = np.maximum(0.0, Z), Z #np.maximum => puts all the 0 and less values to 0 and the others continue been the same \n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        Z, AWb_cache = linear_forward(A_prev, W, b) # linear cache returns the A, W and b we used \n",
    "        A, Z_cache = 1 / (1 + np.exp(-Z)), Z #activation cache return the z we used \n",
    "\n",
    "    elif activation == \"tanh\":\n",
    "        Z, AWb_cache = linear_forward(A_prev, W, b)\n",
    "        A, Z_cache = np.tanh(Z), Z\n",
    "        \n",
    "    cache = (AWb_cache, Z_cache)\n",
    "        \n",
    "    return A, cache\n",
    "\n",
    "A, cache = linear_activation_forward(X, parameters[\"W1\"], parameters[\"b1\"], \"relu\")\n",
    "#cache[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-layer Model forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \n",
    "    caches = [] #list with all caches\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # number of layers in the neural network parameters have W´s and b´s\n",
    "    \n",
    "    \n",
    "    #[LINEAR -> RELU]*(L-1)\n",
    "    for l in range (1,L): # if L => 4  ... l will take (1,2,3)\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\")\n",
    "        caches.append(cache) #append all caches to the list\n",
    "    \n",
    "    #LINEAR -> SIGMOID for the last one\n",
    "    AL, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches\n",
    "\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "#caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 200)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = -(1/m) * np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL))\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    return cost\n",
    "\n",
    "cost = compute_cost(AL, Y)\n",
    "#cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    #derivatives\n",
    "    dW = (1/m) * dZ@A_prev.T\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True) #sum by the rows of dZ with keepdims=True\n",
    "    dA_prev = W.T@dZ\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \n",
    "    linear_cache, activation_cache = cache # linear_cache=> A_prev,W,b  activation_cache=> Z\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        \"\"\"\n",
    "        activation2 = np.zeros((len(activation_cache),1))\n",
    "        for i in range(len(activation_cache)):\n",
    "            if activation_cache[i]>= 0:\n",
    "                activation2[i] = 1\n",
    "            else:\n",
    "                activation2[i] = 0\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        z2 = list(map(lambda x: 1 if x>=0 else 0, activation_cache))\n",
    "        z2 = np.reshape(z2, (-1,1))\n",
    "        \"\"\" \n",
    "        #puts all the values >= 0 to 1 and less than cero in 0\n",
    "        z_new = lambda x: 1 if x>=0 else 0\n",
    "        f = np.vectorize(z_new)  \n",
    "        z2 = f(activation_cache)\n",
    "        dZ = dA * z2 # dA * if z>=0, 1 else 0\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = dA * (1 / (1 + np.exp(-activation_cache))) *(1 - (1 / (1 + np.exp(-activation_cache))))# dA * A(1-A) => \"A-Y\"\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = dA * (1 - np.power(np.tanh(activation_cache), 2))# dA * (1-A**2)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache) \n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "#dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "#dA_prev, dW, db = linear_activation_backward(dAL, cache, \"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    #Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    #(1 line of code)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # =>(a-y)/(a(1-a))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    #(approx. 5 lines)\n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        #(approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dA_prev_temp, current_cache, \"relu\") #or tanh\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \n",
    "    parameters = params.copy()\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)] \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)] \n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)    \n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
